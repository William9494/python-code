{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ§¹ Data Cleaning & Preprocessing â€” Practical Notebook\n",
        "\n",
        "**Generated:** 2025-09-02 01:33 UTC Â· **Target stack:** pandas â‰¥ 2.x, NumPy, SciPy (optional)\n",
        "\n",
        "This notebook is a handsâ€‘on cheat sheet for the data cleaning & preprocessing phase:\n",
        "- Creating a messy dataset\n",
        "- Detecting & unifying missing values\n",
        "- Fixing dtypes (numeric, date/time, categorical)\n",
        "- String cleaning & regex parsing\n",
        "- Handling duplicates\n",
        "- Outliers (IQR, zâ€‘score, winsorization)\n",
        "- Imputation strategies (mean/median/mode, groupwise, ffill/bfill, interpolate)\n",
        "- Category harmonization & binning\n",
        "- Scaling & normalization (standard/minâ€‘max/robust â€” manual)\n",
        "- Date/time feature engineering\n",
        "- Basic text cleaning\n",
        "- Data validation checks\n",
        "- A reusable `.pipe`â€‘based cleaning pipeline template\n",
        "\n",
        "> âš ï¸ **Version notes**\n",
        "> * Avoid `DataFrame.append` (removed in pandas 2.0). Use `pd.concat` instead.\n",
        "> * `DataFrame.applymap` deprecated in 2.1 â€” use `DataFrame.map` or vectorized ops.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from scipy import stats\n",
        "import re\n",
        "\n",
        "# Display options (feel free to tweak)\n",
        "pd.set_option('display.max_rows', 50)\n",
        "pd.set_option('display.max_columns', 50)\n",
        "pd.set_option('display.width', 120)\n",
        "\n",
        "RNG = np.random.default_rng(42)\n",
        "pd.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Create a deliberately messy dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deliberately messy raw data\n",
        "raw = pd.DataFrame({\n",
        "    '  Customer Name  ': ['  Alice  ', 'Bob', 'ChloÃ©', 'dAn', 'Eve', 'Bob', 'NA', None],\n",
        "    'Age ': ['29', '  35', 'twenty', '42 ', None, '35', '27', '27'],\n",
        "    'signup_date': ['2024-01-05', '05/02/2024', '2024/03/15', '15-04-2024', '2024-05-20', '2024-05-20', '', '2024-07-01'],\n",
        "    'revenue($)': ['1,200.50', '$850', 'â‚¬900', '1 100,00', 'â€”', '850', '1200.50', 'null'],\n",
        "    'city': ['New York', 'newyork', 'NY', 'San Francisco', 'SF', 'San-Francisco', 'n/a', 'New  York '],\n",
        "    'phone': ['+1 (212) 555-0198', '212.555.0199', '212-555-0198', None, ' 212 555 0197 ', '2125550199', 'N/A', '212/555/0196'],\n",
        "    'segment': ['Enterprise', 'SMB', 'smb', 'Mid-Market', 'Mid Market', 'SMB', 'enterprise', 'SMB'],\n",
        "    'notes': ['Good client!!', ' late payer ', None, 'VIP;  high priority', 'Refund issued â€“ check', 'ðŸ˜Š Loyal', 'NA', '  ']\n",
        "})\n",
        "raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Quick scan & profiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw.describe(include='all', datetime_is_numeric=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw.isna().sum(), raw.eq('').sum(), raw.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Standardize column names (trim, snake_case, ASCII)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "\n",
        "def normalize_colnames(cols):\n",
        "    out = []\n",
        "    for c in cols:\n",
        "        c = c.strip()\n",
        "        # Normalize unicode -> ASCII base\n",
        "        c = unicodedata.normalize('NFKD', c).encode('ascii', 'ignore').decode('ascii')\n",
        "        # Snake case-ish\n",
        "        c = re.sub(r\"[^0-9a-zA-Z]+\", \"_\", c).strip('_').lower()\n",
        "        out.append(c)\n",
        "    return out\n",
        "\n",
        "clean = raw.copy()\n",
        "clean.columns = normalize_colnames(clean.columns)\n",
        "clean.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Unify missing value tokens to `NaN`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MISSING_TOKENS = {\"na\", \"n/a\", \"null\", \"none\", \"â€”\", \"-\", \"\"}\n",
        "\n",
        "clean = clean.replace({col: {tok: np.nan for tok in MISSING_TOKENS} for col in clean.columns}, regex=False)\n",
        "# Also strip whitespace in object cols\n",
        "for c in clean.select_dtypes(include='object'):\n",
        "    clean[c] = clean[c].str.strip()\n",
        "\n",
        "clean.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Fix data types (numeric/date/categorical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Age: coerce to numeric; invalid -> NaN\n",
        "clean['age'] = pd.to_numeric(clean['age'], errors='coerce')\n",
        "\n",
        "# Dates: try multiple formats robustly; coerce invalid to NaT\n",
        "clean['signup_date'] = pd.to_datetime(clean['signup_date'], errors='coerce', dayfirst=True, infer_datetime_format=True)\n",
        "\n",
        "# Revenue: remove currency/locale artifacts -> float\n",
        "rev = (clean['revenue']\n",
        "         .str.replace(r\"[\\s,]\", \"\", regex=True)        # spaces & thousands separators\n",
        "         .str.replace(r\"^[\\$â‚¬]\", \"\", regex=True)        # leading currency symbol\n",
        "         .str.replace(r\",\" , \"\", regex=False)           # leftover commas (safety)\n",
        "      )\n",
        "# Handle European comma decimal (e.g., 1 100,00 -> 1100.00 after removing spaces, then replace last comma->dot)\n",
        "rev = rev.str.replace(r\"(?P<num>\\d+),(\\d{2})$\", lambda m: f\"{m.group('num')}.{m.group(2)}\", regex=True)\n",
        "clean['revenue_usd'] = pd.to_numeric(rev, errors='coerce')\n",
        "\n",
        "# Categorical candidates\n",
        "for col in ['city', 'segment']:\n",
        "    clean[col] = clean[col].astype('string').str.strip()\n",
        "\n",
        "clean.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) String cleaning & regex parsing (names, phones, categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Names: title case after trimming and unicode normalization\n",
        "clean['customer_name'] = (clean['customer_name']\n",
        "    .fillna('')\n",
        "    .str.normalize('NFKD')\n",
        "    .str.encode('ascii','ignore').str.decode('ascii')\n",
        "    .str.strip()\n",
        "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "    .str.title()\n",
        ")\n",
        "\n",
        "# Phones: keep only digits; standardize as E.164-like for US (assume missing +1)\n",
        "clean['phone_digits'] = (clean['phone']\n",
        "    .astype('string')\n",
        "    .str.replace(r\"\\D+\", \"\", regex=True)\n",
        ")\n",
        "clean['phone_std'] = clean['phone_digits'].where(clean['phone_digits'].str.len()>0)\n",
        "clean.loc[clean['phone_std'].str.len()==10, 'phone_std'] = '+1' + clean['phone_std']\n",
        "clean.loc[clean['phone_std'].str.len()==11, 'phone_std'] = '+' + clean['phone_std']\n",
        "\n",
        "# City harmonization\n",
        "CITY_MAP = {\n",
        "    'newyork':'New York', 'ny':'New York', 'new  york':'New York',\n",
        "    'san francisco':'San Francisco', 'san-francisco':'San Francisco', 'sf':'San Francisco'\n",
        "}\n",
        "clean['city_norm'] = (clean['city']\n",
        "    .str.lower()\n",
        "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "    .str.strip()\n",
        "    .map(lambda x: CITY_MAP.get(x, x.title()) if isinstance(x, str) else x)\n",
        ")\n",
        "\n",
        "# Segment harmonization & categories\n",
        "SEG_MAP = {'smb':'SMB', 'mid market':'Mid-Market', 'enterprise':'Enterprise'}\n",
        "clean['segment_norm'] = (clean['segment']\n",
        "    .str.lower().str.strip()\n",
        "    .map(lambda x: SEG_MAP.get(x, x.title()))\n",
        "    .astype('category')\n",
        ")\n",
        "\n",
        "clean[['customer_name','phone','phone_std','city','city_norm','segment','segment_norm']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Detect & remove duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Consider duplicates by name + phone_std + signup_date\n",
        "dupe_mask = clean.duplicated(subset=['customer_name','phone_std','signup_date'], keep='first')\n",
        "clean['is_duplicate'] = dupe_mask\n",
        "\n",
        "clean_dupes_removed = clean.loc[~dupe_mask].copy()\n",
        "clean_dupes_removed[['customer_name','phone_std','signup_date','is_duplicate']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Outliers (IQR / z-score / winsorization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num = clean_dupes_removed[['age','revenue_usd']].copy()\n",
        "\n",
        "# IQR method\n",
        "iqr = num.quantile(0.75) - num.quantile(0.25)\n",
        "lo = num.quantile(0.25) - 1.5 * iqr\n",
        "hi = num.quantile(0.75) + 1.5 * iqr\n",
        "bounds_iqr = pd.DataFrame({'low': lo, 'high': hi})\n",
        "\n",
        "# z-score method\n",
        "z = num.apply(lambda s: np.abs(stats.zscore(s, nan_policy='omit')))\n",
        "zflag = (z > 3)\n",
        "\n",
        "# Winsorization (cap extremes at 5th/95th percentiles)\n",
        "wins = num.copy()\n",
        "q05 = wins.quantile(0.05)\n",
        "q95 = wins.quantile(0.95)\n",
        "wins = wins.clip(lower=q05, upper=q95, axis=1)\n",
        "\n",
        "bounds_iqr, zflag.sum(), wins.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Imputation strategies (mean/median/mode, ffill/bfill, interpolate, groupwise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "impute = clean_dupes_removed.copy()\n",
        "\n",
        "# Numeric global strategies\n",
        "impute['age_mean'] = impute['age'].fillna(impute['age'].mean())\n",
        "impute['age_median'] = impute['age'].fillna(impute['age'].median())\n",
        "\n",
        "# Groupwise median by segment\n",
        "impute['age_by_segment'] = impute.groupby('segment_norm')['age']                               .transform(lambda s: s.fillna(s.median()))\n",
        "\n",
        "# Revenue interpolation (if time-indexed)\n",
        "impute_sorted = impute.sort_values('signup_date').set_index('signup_date')\n",
        "impute_sorted['revenue_interp'] = impute_sorted['revenue_usd'].interpolate(method='time')\n",
        "\n",
        "# Categorical: fill with mode\n",
        "mode_seg = impute['segment_norm'].mode(dropna=True)\n",
        "if not mode_seg.empty:\n",
        "    impute['segment_imputed'] = impute['segment_norm'].cat.add_categories(['_MISSING_'])\n",
        "    impute['segment_imputed'] = impute['segment_imputed'].fillna(mode_seg.iloc[0])\n",
        "else:\n",
        "    impute['segment_imputed'] = impute['segment_norm']\n",
        "\n",
        "# Forward/backward fill for phone\n",
        "impute['phone_std_ffill'] = impute['phone_std'].ffill().bfill()\n",
        "\n",
        "impute[['age','age_mean','age_median','age_by_segment','revenue_usd']].head(), impute_sorted[['revenue_usd','revenue_interp']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Binning (equal width & quantile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create synthetic 'orders' to demonstrate\n",
        "orders = pd.DataFrame({'amount': RNG.normal(100, 30, size=1000)})\n",
        "orders['amount'] = orders['amount'].clip(lower=0)\n",
        "\n",
        "orders['amount_bin_width'] = pd.cut(orders['amount'], bins=[0,50,100,150,200, np.inf], right=False)\n",
        "orders['amount_bin_quantile'] = pd.qcut(orders['amount'], q=5)\n",
        "\n",
        "orders['amount_bin_width'].value_counts().sort_index(), orders['amount_bin_quantile'].value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Scaling & normalization (manual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scale_df = clean_dupes_removed[['age','revenue_usd']].astype(float)\n",
        "\n",
        "# Standardization\n",
        "standardized = (scale_df - scale_df.mean()) / scale_df.std(ddof=0)\n",
        "\n",
        "# Min-Max scaling\n",
        "minmax = (scale_df - scale_df.min()) / (scale_df.max() - scale_df.min())\n",
        "\n",
        "# Robust scaling (IQR)\n",
        "iqr = scale_df.quantile(0.75) - scale_df.quantile(0.25)\n",
        "robust = (scale_df - scale_df.median()) / iqr.replace(0, np.nan)\n",
        "\n",
        "standardized.head(), minmax.head(), robust.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) Date/time features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dt = clean_dupes_removed.copy()\n",
        "\n",
        "# Ensure datetime\n",
        "dt['signup_date'] = pd.to_datetime(dt['signup_date'], errors='coerce')\n",
        "\n",
        "# Extract features\n",
        "features = pd.DataFrame({\n",
        "    'year': dt['signup_date'].dt.year,\n",
        "    'month': dt['signup_date'].dt.month,\n",
        "    'day': dt['signup_date'].dt.day,\n",
        "    'dow': dt['signup_date'].dt.day_name(),\n",
        "    'week': dt['signup_date'].dt.isocalendar().week.astype('Int64'),\n",
        "    'is_month_start': dt['signup_date'].dt.is_month_start,\n",
        "    'is_month_end': dt['signup_date'].dt.is_month_end,\n",
        "})\n",
        "features.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13) Basic text cleaning (punctuation, emoji, length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = clean_dupes_removed[['notes']].copy()\n",
        "text['notes'] = text['notes'].fillna('')\n",
        "\n",
        "# Lowercase, trim, collapse spaces\n",
        "text['notes_clean'] = (text['notes']\n",
        "    .str.lower()\n",
        "    .str.strip()\n",
        "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
        ")\n",
        "\n",
        "# Remove punctuation & emoji (basic heuristic)\n",
        "text['notes_alpha'] = text['notes_clean'].str.replace(r\"[^a-z0-9\\s]\", \"\", regex=True)\n",
        "\n",
        "# Length features\n",
        "text['char_len'] = text['notes_clean'].str.len()\n",
        "text['word_count'] = text['notes_clean'].str.split().map(len)\n",
        "\n",
        "text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14) Data validation checks (assertions & diagnostics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "valid = clean_dupes_removed.copy()\n",
        "\n",
        "errors = {}\n",
        "\n",
        "# Example expectations\n",
        "errors['age_nonnegative'] = valid.loc[~valid['age'].fillna(0).between(0, 120)]\n",
        "errors['revenue_nonnegative'] = valid.loc[~valid['revenue_usd'].fillna(0).ge(0)]\n",
        "errors['phone_format'] = valid.loc[~valid['phone_std'].fillna('').str.match(r\"^\\+\\d{11}$|^$\", na=False)]\n",
        "\n",
        "# Primary key uniqueness (name+phone on same date)\n",
        "pk_dupes = valid.duplicated(subset=['customer_name','phone_std','signup_date'], keep=False)\n",
        "errors['primary_key_dupes'] = valid.loc[pk_dupes]\n",
        "\n",
        "{k: v.shape[0] for k,v in errors.items()}, {k: v.head(3) for k,v in errors.items() if not v.empty}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15) Save cleaned data (CSV/Parquet) â€” optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to save\n",
        "# clean_dupes_removed.to_csv('clean_customers.csv', index=False)\n",
        "# clean_dupes_removed.to_parquet('clean_customers.parquet', index=False)\n",
        "\n",
        "'Not saved (demo)'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16) Reusable cleaning pipeline template (.pipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import unicodedata, re, numpy as np, pandas as pd\n",
        "\n",
        "MISSING_TOKENS = {\"na\",\"n/a\",\"null\",\"none\",\"â€”\",\"-\",\"\"}\n",
        "CITY_MAP = {\n",
        "    'newyork':'New York', 'ny':'New York', 'new  york':'New York',\n",
        "    'san francisco':'San Francisco', 'san-francisco':'San Francisco', 'sf':'San Francisco'\n",
        "}\n",
        "SEG_MAP = {'smb':'SMB', 'mid market':'Mid-Market', 'enterprise':'Enterprise'}\n",
        "\n",
        "\n",
        "def step_standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    def norm(c):\n",
        "        c = c.strip()\n",
        "        c = unicodedata.normalize('NFKD', c).encode('ascii','ignore').decode('ascii')\n",
        "        c = re.sub(r\"[^0-9a-zA-Z]+\", \"_\", c).strip('_').lower()\n",
        "        return c\n",
        "    df = df.copy()\n",
        "    df.columns = [norm(c) for c in df.columns]\n",
        "    return df\n",
        "\n",
        "\n",
        "def step_unify_missing(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    repl = {col: {tok: np.nan for tok in MISSING_TOKENS} for col in df.columns}\n",
        "    df = df.replace(repl, regex=False)\n",
        "    for c in df.select_dtypes(include='object'):\n",
        "        df[c] = df[c].str.strip()\n",
        "    return df\n",
        "\n",
        "\n",
        "def step_fix_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    if 'age' in df:\n",
        "        df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
        "    if 'signup_date' in df:\n",
        "        df['signup_date'] = pd.to_datetime(df['signup_date'], errors='coerce', dayfirst=True, infer_datetime_format=True)\n",
        "    if 'revenue' in df or 'revenue($)' in df:\n",
        "        col = 'revenue' if 'revenue' in df else 'revenue($)'\n",
        "        rev = (df[col].astype('string')\n",
        "                 .str.replace(r\"[\\s,]\", \"\", regex=True)\n",
        "                 .str.replace(r\"^[\\$â‚¬]\", \"\", regex=True))\n",
        "        rev = rev.str.replace(r\"(?P<num>\\d+),(\\d{2})$\", lambda m: f\"{m.group('num')}.{m.group(2)}\", regex=True)\n",
        "        df['revenue_usd'] = pd.to_numeric(rev, errors='coerce')\n",
        "    return df\n",
        "\n",
        "\n",
        "def step_clean_strings(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    if 'customer_name' in df:\n",
        "        df['customer_name'] = (df['customer_name'].fillna('')\n",
        "            .str.normalize('NFKD').str.encode('ascii','ignore').str.decode('ascii')\n",
        "            .str.strip().str.replace(r\"\\s+\",\" \", regex=True).str.title())\n",
        "    if 'phone' in df:\n",
        "        phone = df['phone'].astype('string').str.replace(r\"\\D+\", \"\", regex=True)\n",
        "        std = phone.where(phone.str.len()>0)\n",
        "        std = std.mask(std.str.len()==10, '+1' + std)\n",
        "        std = std.mask(std.str.len()==11, '+' + std)\n",
        "        df['phone_std'] = std\n",
        "    if 'city' in df:\n",
        "        df['city_norm'] = (df['city'].astype('string').str.lower().str.replace(r\"\\s+\",\" \", regex=True).str.strip()\n",
        "            .map(lambda x: CITY_MAP.get(x, x.title()) if isinstance(x,str) else x))\n",
        "    if 'segment' in df:\n",
        "        df['segment_norm'] = (df['segment'].astype('string').str.lower().str.strip()\n",
        "            .map(lambda x: SEG_MAP.get(x, x.title())).astype('category'))\n",
        "    return df\n",
        "\n",
        "\n",
        "def step_drop_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    key_cols = [c for c in ['customer_name','phone_std','signup_date'] if c in df]\n",
        "    if key_cols:\n",
        "        return df.drop_duplicates(subset=key_cols, keep='first')\n",
        "    return df\n",
        "\n",
        "\n",
        "def clean_pipeline(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    return (df\n",
        "        .pipe(step_standardize_columns)\n",
        "        .pipe(step_unify_missing)\n",
        "        .pipe(step_fix_dtypes)\n",
        "        .pipe(step_clean_strings)\n",
        "        .pipe(step_drop_duplicates)\n",
        "    )\n",
        "\n",
        "# Demo: apply to the original raw data\n",
        "cleaned = clean_pipeline(raw)\n",
        "cleaned.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Wrapâ€‘up\n",
        "You now have a working notebook that demonstrates practical, reproducible dataâ€‘cleaning patterns using idiomatic pandas. Duplicate the functions in **Section 16** into your project and adapt the mapping dictionaries (cities, segments, etc.) to your domain."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}